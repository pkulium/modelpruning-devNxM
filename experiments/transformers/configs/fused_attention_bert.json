{
    "bert.encoder.layer.0.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.0.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.0.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.0.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.1.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.1.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.1.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.1.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.2.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.2.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.2.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.2.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.3.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.3.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.3.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.3.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.4.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.4.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.4.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.4.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.5.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.5.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.5.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.5.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.6.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.6.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.6.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.6.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.7.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.7.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.7.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.7.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.8.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.8.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.8.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.8.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.9.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.9.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.9.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.9.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.10.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.10.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.10.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.10.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.11.attention.self.key": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.11.attention.self.value": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.11.attention.self.query": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    },
    "bert.encoder.layer.11.attention.output.dense": {
        "type": "NxM_QUANT",
        "args": {
            "N": 4,
            "M": 2,
            "precision": 8,
            "norm": 2,
            "chunk_size": -1,
            "groups": 1
        }
    }
}